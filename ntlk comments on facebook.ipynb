{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc8c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425a2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning and sentiment analysis\n",
    "import html\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa0ef55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prime Deals\\nhttps://t.co/A7qRIiddKK\\n#powertools'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text example with newlines \"\\n\"\n",
    "text = 'Prime Deals\\nhttps://t.co/A7qRIiddKK\\n#powertools'\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88b903e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prime Deals https://t.co/A7qRIiddKK #powertools'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# substitute newlines with spaces (regular expression match and substitute)\n",
    "re.sub('\\n+', ' ', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5ad9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prime Deals  #powertools'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove URLs (links are not for sentiment analysis)\n",
    "re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1e5b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a smiley face ðŸ˜‚'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyze text content, not encoded emoji etc.\n",
    "semoji = 'This is a smiley face \\U0001f602'\n",
    "semoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81fdc38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a smiley face '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove non-ASCII characters to leave only the text\n",
    "# CAUTION: if needeng to analyze multilingual text, modify this as in the answer below for example\n",
    "# https://stackoverflow.com/questions/51784964/remove-emojis-from-multilingual-unicode-text/51785357#51785357\n",
    "semoji.encode('ascii', 'ignore').decode('ascii')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "398e7e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food &amp; culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. https://t.co/iEzvKd0LZu\n"
     ]
    }
   ],
   "source": [
    "# example text of some Tweet: more complex html encoding\n",
    "text = '\\\"You don\\u2019t need a car to enjoy Melbourne\\u2019s food &amp; culture scene \\u2013 but you don\\u2019t want to ride everywhere, either. If only there was something in between. Oh, wait \\u2013 there is: an eBike. https://t.co/iEzvKd0LZu'\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a79f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food & culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. \n"
     ]
    }
   ],
   "source": [
    "# this string has special symbols encoded (\"escaped characters\")\n",
    "# construct the original text (note for example: \"&amp\" becoming \"&\")\n",
    "# rememberimng to also remove new lines first\n",
    "text_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', text)))\n",
    "print(text_unesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbfe7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# string cleanup function: collect the steps so far\n",
    "def text_cleanup_init(s):\n",
    "    s_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', text)))\n",
    "    s_noemoji = s_unesc.encode('ascii', 'ignore').decode('ascii')\n",
    "    # normalize to lowercase\n",
    "    return s_noemoji.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde04a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food &amp; culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. https://t.co/iEzvKd0LZu\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a526929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"you dont need a car to enjoy melbournes food & culture scene  but you dont want to ride everywhere, either. if only there was something in between. oh, wait  there is: an ebike. \n"
     ]
    }
   ],
   "source": [
    "text_clean_init = text_cleanup_init(text)\n",
    "print(text_clean_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce4901ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['``',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'need',\n",
       " 'a',\n",
       " 'car',\n",
       " 'to',\n",
       " 'enjoy',\n",
       " 'melbournes',\n",
       " 'food',\n",
       " '&',\n",
       " 'culture',\n",
       " 'scene',\n",
       " 'but',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'to',\n",
       " 'ride',\n",
       " 'everywhere',\n",
       " ',',\n",
       " 'either',\n",
       " '.',\n",
       " 'if',\n",
       " 'only',\n",
       " 'there',\n",
       " 'was',\n",
       " 'something',\n",
       " 'in',\n",
       " 'between',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'wait',\n",
       " 'there',\n",
       " 'is',\n",
       " ':',\n",
       " 'an',\n",
       " 'ebike',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the string (NLTK tools)\n",
    "word_tokens = word_tokenize(text_clean_init)\n",
    "word_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4885e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shan', 'any', 'weren', \"won't\", 'whom', \"wasn't\", \"haven't\", \"mustn't\", 'that', 'below', 'aren', 'your', 'hasn', 'why', 've', 'above', 'some', 'them', 'or', 'same', 'now', 'ain', 'over', \"shouldn't\", \"you've\", 'a', 'should', 'having', \"hadn't\", 'on', 'him', 'did', 'mustn', 'not', 'most', 'again', 'there', 'between', \"don't\", 'in', 'his', 'doesn', \"needn't\", 'isn', \"that'll\", 't', 'against', 'with', \"weren't\", \"you'll\", 'and', 'was', 'about', 'hadn', 'yourself', 'further', 'were', 'other', 'up', 'myself', 'herself', 'than', 'too', 'himself', 'those', 'from', 'does', 'mightn', 'for', 'haven', 'are', 'wouldn', 'during', 'ma', 'out', 'itself', 'the', 'when', 'few', 'which', 'i', \"didn't\", 'their', \"couldn't\", 'our', 'each', \"it's\", 'off', 'me', 're', 'he', 'until', 'm', 'is', 'theirs', 'by', 'after', \"isn't\", 'only', 'because', 'being', 's', 'shouldn', 'this', \"mightn't\", 'before', \"she's\", \"aren't\", 'yourselves', 'under', 'wasn', 'y', 'it', 'through', \"hasn't\", 'so', 'yours', 'these', 'won', 'ours', 'its', 'am', 'had', 'couldn', 'my', \"you're\", 'own', 'such', 'needn', 'doing', 'o', \"should've\", 'an', 'she', 'at', 'nor', 'didn', 'once', 'to', 'will', 'of', 'both', 'down', 'then', 'how', 'd', 'they', 'be', 'have', 'into', 'do', 'where', \"shan't\", 'very', 'but', 'don', 'as', 'can', 'you', 'll', 'all', 'we', 'if', 'who', 'themselves', 'while', \"you'd\", 'hers', 'ourselves', 'what', 'been', 'no', 'her', \"wouldn't\", 'has', \"doesn't\", 'more', 'here', 'just'}\n"
     ]
    }
   ],
   "source": [
    "# we want to remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1f1295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# and also any punctuation marks\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f3a961f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dont',\n",
       " 'need',\n",
       " 'car',\n",
       " 'enjoy',\n",
       " 'melbournes',\n",
       " 'food',\n",
       " 'culture',\n",
       " 'scene',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'ride',\n",
       " 'everywhere',\n",
       " 'either',\n",
       " 'something',\n",
       " 'oh',\n",
       " 'wait',\n",
       " 'ebike']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stop words and punctuation, also any non-alphanumeric strings (not words)\n",
    "word_tokens_filt = [w for w in word_tokens if (w not in stop_words) and (w not in string.punctuation) and (w.isalnum())]\n",
    "word_tokens_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1e9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dont need car enjoy melbournes food culture scene dont want ride everywhere either something oh wait ebike'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct clean text\n",
    "text_clean = ' '.join(word_tokens_filt).lower()\n",
    "text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a194ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, augment our text cleanup function\n",
    "\n",
    "# no need to collect stop-words every time we run the function; pass the set as an argument\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_cleanup(s, stop_words):\n",
    "    s_unesc = html.unescape(re.sub(r\"http\\S+\", \"\", re.sub('\\n+', ' ', s)))\n",
    "    s_noemoji = s_unesc.encode('ascii', 'ignore').decode('ascii')\n",
    "    # normalize to lowercase and tokenize\n",
    "    wt = word_tokenize(s_noemoji.lower())\n",
    "    \n",
    "    # filter word-tokens\n",
    "    wt_filt = [w for w in wt if (w not in stop_words) and (w not in string.punctuation) and (w.isalnum())]\n",
    "    \n",
    "    # return clean string\n",
    "    return ' '.join(wt_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3b8c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"You donâ€™t need a car to enjoy Melbourneâ€™s food &amp; culture scene â€“ but you donâ€™t want to ride everywhere, either. If only there was something in between. Oh, wait â€“ there is: an eBike. https://t.co/iEzvKd0LZu\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94e968f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dont need car enjoy melbournes food culture scene dont want ride everywhere either something oh wait ebike\n"
     ]
    }
   ],
   "source": [
    "text_clean = text_cleanup(text, stop_words)\n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7d40898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentiment analysis:\n",
    "# install textblob package (simplifies working with nltk)\n",
    "# in terminal:\n",
    "#     conda install -c conda-forge textblob\n",
    "#\n",
    "# alternatively, right here in the notebook (uncomment the next two lines and run):\n",
    "#import sys\n",
    "#!conda install -c conda-forge --yes --prefix {sys.prefix} textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3818c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "494d8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob is built upon NLTK and provides an easy interface to the NLTK library\n",
    "# https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library\n",
    "analysis = TextBlob(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb7e0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.4, subjectivity=0.5)\n"
     ]
    }
   ],
   "source": [
    "## polarity: -1 to 1\n",
    "## subjectivity: 0 to 1 (1 is personal opinion, 0 more factual support)\n",
    "print(analysis.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38d682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write a function that takes an input string and displays sentiment analysis;\n",
    "# then come up with some example sentenses to test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59799cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
