{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31395c11",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect relevant content through the Reddit API.\n",
    "import json\n",
    "import praw\n",
    "# PRAW documentation:\n",
    "#  https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0de9f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: enter proper access credential in the config-file;\n",
    "# follow instructions in reddit_credentials_verify.ipynb\n",
    "import config_reddit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19c6a2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish an API connection and verify read-only access\n",
    "reddit = praw.Reddit(user_agent=f\"Exploration script by /u/{config_reddit.user_name}\",\n",
    "                     client_id=config_reddit.app_id,\n",
    "                     client_secret=config_reddit.app_secret)\n",
    "reddit.read_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f9d2140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a subreddit of interest\n",
    "# MODIFY this to what you prefer to analyze\n",
    "#\n",
    "# Example (take the string from the ending-part of the subreddit URL):\n",
    "#  https://www.reddit.com/r/Business Intelligence/\n",
    "query_subreddit = 'BusinessIntelligence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cce182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide how many top-\"hot\" posts to query\n",
    "nposts = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "caf72149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect ids of the top posts within the chosen subreddit\n",
    "post_ids = []\n",
    "subreddit = reddit.subreddit(query_subreddit)\n",
    "for p in subreddit.hot(limit = nposts):\n",
    "    post_ids.append(p.id)\n",
    "# check how many posts (submissions) were collected\n",
    "len(post_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f832828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon offer - would you take it?\n",
      "Hey BI fellows , recently got an offer from Amazon for a L5 BI engineer role , location is remote , 140k Base and 190k TTC. I have about 9 years of experience in this field. Would you accept this offer ? I currently made around 160k Base and have 40k RSU vesting in 2 years so Amazon offer is just like a little raise for me, but my current company has 25 days PTO and Amazon first year only have 16… \n",
      "\n",
      "Big part of me wanting to join Amazon because it’s FAANG and I like the name, but I heard all kind of scary story about Amazon WLB and they have annual target URA that force people to leave.. \n",
      "\n",
      "If anyone can share any insight that will be great.  Thanks all!\n"
     ]
    }
   ],
   "source": [
    "# example post details\n",
    "post_details = reddit.submission(id = post_ids[1])\n",
    "print(post_details.title)\n",
    "print(post_details.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a2e2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide how many top comments to query per post;\n",
    "# NOTE: larger number of comments may dilute the content (irrelevant text)\n",
    "ncomments = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f608e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect post data\n",
    "def collect_post_data(post_id, ncomments, reddit):\n",
    "    psubm = reddit.submission(id = post_id)\n",
    "    pdata = {'id': post_id, 'title': psubm.title, 'text': psubm.selftext}\n",
    "    \n",
    "    # collect first- and second-level comments\n",
    "    pcomm = []\n",
    "    psubcomm = []\n",
    "    psubm.comments.replace_more(limit = ncomments)\n",
    "    for top_comment in psubm.comments:\n",
    "        pcomm.append(top_comment.body)\n",
    "        for lev2_comment in top_comment.replies:\n",
    "            psubcomm.append(lev2_comment.body)\n",
    "    \n",
    "    # assemble the data together\n",
    "    pdata['comments_lev1'] = pcomm\n",
    "    pdata['comments_lev2'] = psubcomm\n",
    "    \n",
    "    return pdata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e23ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect information for each post\n",
    "posts_all = [collect_post_data(pid, ncomments, reddit) for pid in post_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0a44817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save collected data to json file\n",
    "file_out = f\"raw_post_comment_data.json\"\n",
    "with open(file_out, mode='w') as f:\n",
    "    f.write(json.dumps(posts_all, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e5c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
